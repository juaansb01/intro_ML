{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## **EMBEDDINGS**"
      ],
      "metadata": {
        "id": "I3iXRQYTbQGD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cuando hablamos de **embeddings** (o incrustaciones), nos estamos refiriendo a la representación de palabras como vectores, las cuales tienen como objetivo principal representar el signifcado de las palabras. Cuando nos ceñimos al ámbito de NLP (Natural Language Processing), **word embedding** y **textual embedding** son las técnicas usadas para representar palabras y textos de un espacio vectorial, lo que permite que las computadoras comprendan y trabajen con texto de una manera más efectiva. <br> Veamos ahora las principales características de estas técnicas.\n",
        "* **Word Embedding**: conversión de palabras individuales en vectores numéricos. Las palabras similares en significado están cerca unas de otras en el espacio vectorial.\n",
        ">* **Word2Vec**, **GloVe**, **FastText** son algoritmos entrenados con grandes corpues de texto no etiquetado para aprender representaciones vectoriales de palabras.\n",
        ">*Permiten caputrar la semántica y similitud entre palabras. Esto es útil a la hora de recuperar información de un texto, efectuar un análisis de sentimientos, traducción automática o detectar la similitud entre palabras.\n",
        "* **Contextual Embedding**: representación de textos completos como vectores de un espacio vectorial. A diferencia de las incrustaciones de palabras, un sólo vector captura el significado global del texto.\n",
        ">* Se usan distintas técnicas como la promediación de las incrustaciones de las palabras en un texto o **BERT**, un algoritmo que genera representaciones contextuales, sobre el que hablaremos más adelante.\n",
        ">* Es útil para la clasificación de textos, la agrupación de documentos según sus temas, recuperar información de un texto o la generación de resúmenes."
      ],
      "metadata": {
        "id": "9wcZBhdQbh_H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **AUTOREGRESSIVE vs AUTOENCODING MODELS**"
      ],
      "metadata": {
        "id": "pUKXe7ik7_9z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dentro de los modelos que se centran en el *contextual embedding*, vamos a ver los siguientes paradigmas de arquitectura, viendo sus principales similitudes o diferencias.\n",
        "* **Autoregressive models (AR)**: cuando hablamos sobre NLP, estos modelos se usan para generar textos y predecir palabras en una secuencia basándose en el contexto anterior de la misma. Para predecir, estiman la distribución de probabilidad de un corpus de texto. Estos modelos sólo están entrenados para codificar un contexto unidireccional, lo cual supone una de sus principales desventajas.\n",
        "> Se centran en medir la correlación entre observaciones en pasos de tiempo anteriores para predecir el valor del siguiente paso de tiempo. Cuanto mayor sea esta correlacióm, más probable es que el pasado prediga el futuro, o más bien, mayor será el valor que se ponderaá durante el entrenamiento de **deep learning**.\n",
        "* **Autoencoding models (AE)**: estos modelos se usan normalmente para tareas de comprensión de contenidos (análisis de sentimientos, clasificación de textos o respuesta extractiva a preguntas). Permiten utilizar contextos bidireccionales para la reconstrucción, lo cual es muy importante a la hora de guardar toda la información posible.\n",
        ">Estos modelos de codificador automático tienen como objetivo de entrenamiento común el modelado de lenguaje enmascarado (MLM), el cual se basa en la idea de recuperar los tokens originales de la versión corrupta, pues ciertos tokens han sido reemplazados por un símbolo especial. <br>Suponer que los tokens enmascarados son independientes entre sí y el *fine tuning* previo al entrenamiento son las principales desventajas de estos modelos.\n",
        "* **Encoder-Decoder or Seq2Seq models**: utilizan tanto un codifcador como un decodificador, tratando cada tarea como conversión/generación de secuencia a secuencia (texto a texto, texto a imagen o imagen a texto). El codificador genera una representación de alta dimensión a partir de la entrada dada, y con esta representación el decodificador genera una salida similar a la entrada. Se utilizan normalmente para tareas que requieran tanto comprensión como generación de contenido.\n",
        ">En el caso de la clasificación de texto, el codificador toma texto como entrada y el decodificador genera etiquetas de texto en lugar de clasificarlas. Tenemos que el codificador aprende qué partes de las entradas son importantes y las pasa a la representación, mientras que los aspectos menos importantes quedan fuera. Debido a que se entrenan juntos el codificador y el decodificador se crea un modelo secuencia a secuencia, ya que si se entrenara sólo una parte obtendríamos o bien un **modelo AR** (decodificador) o bien un **modelo AE** (codificador)."
      ],
      "metadata": {
        "id": "QfcZq4eHC8PP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **TRANSFORMERS**"
      ],
      "metadata": {
        "id": "xVNWcglLEVyT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Esta nueva red neuronal fue introducida a partir del artículo [*Attention is all you need*](https://arxiv.org/abs/1706.03762), y es una arquitectura de tipo **codificador-decodificador** basada en la atención (en principio le otorga una ventana de contexto infinita). La principal diferencia con los modelos **Seq2Seq** existentes anteriormente es que no utilizan ninguna red recurrente, puesto que se centran en la atención para hacer sus predicciones. Queremos comprender la arquitectura **Transformer** original. Para ello, veremos que esencialmente se trata de una arquitectura codificador-decodificador que realiza un aprendizaje secuencia a secuencia:\n",
        "* N segmentos de codificador que toman entradas y las codifican en una representación intermedia de dimensiones superiores. Toman como entrada el estado previamente codificado o la secuencia fuente.\n",
        ">Garantizan que las entradas se conviertan en una representación intermedia abstracta y de alta dimensión.\n",
        "* N segmentos de decodificador que toman el estado codificado final como entrada, así como la salida del segmento de decodificador anterior o la secuencia de entrada de destino. En ocasiones se usa el **teacher forcing**, una técnica que proporciona las secuencias objetivo reales y así acelerar y estabilizar el proceso de entrenamiento.\n",
        ">Toman la representación intermedia dada por el codificador, proporcionando contexto sobre la entrada y la secuencia de destino, y garantizan que se puedan predecir secuencias apropiadas.\n",
        "Por tanto, el modelo **Transformer** original es un modelo de secuencia a secuencia (**Seq2Seq**).\n",
        "<p align=\"center\">\n",
        " <img width=\"500\" height=\"570\" src=\"https://miro.medium.com/v2/resize:fit:720/format:webp/1*iJ1pdYHWt6RepLyeySnZAQ.png\">\n",
        "</p>"
      ],
      "metadata": {
        "id": "J-L7Heuo4KNQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Basándonos en la imagen anterior, y para comprender mejor el funcionamiento de los **transformers**, queremos explicar más en detalle tanto su estructura como su flujo de trabajo."
      ],
      "metadata": {
        "id": "8I_e7D5QK_rt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. **Capa codificadora**<br>\n",
        "Esta capa mapea todas las secuencias de entrada en una representación continua abstracta que contiene la información aprendida para esa secuencia completa.\n",
        "Contiene las siguientes capas.\n",
        "* Incrustaciones de entrada. Se empieza enviando información a la capa de incrustación de palabras. Se puede considerar como una matriz que almacena la representación vectorial aprendida de cada palabra. Esta representación se refiere a un vector con valores continuos que representa esa palabra.\n",
        "* Codificación posicional. A diferencia de las RNN (Recurrent Neural Network) no disponemos de información posicional, por lo que debemos añadirla a las incrustaciones. Para ello, agregamos los vectores de posición (calculados con funciones seno y coseno) a sus correspondientes incrustaciones de entrada.\n",
        "* Atención multidireccional. Los mecansimos de atención observan una secuencia de entrada y deciden en cada paso qué otras partes de la secuencia son importantes. En este caso, aplica un mecanismo de atención específico llamado **autoatención**, el cual permite a los modelos asociar cada palabra de la entrada con otras palabras. Para lograr la autoatención, el **transformer** sigue los siguientes pasos.\n",
        ">* Alimentamos la entrada en 3 capas distintas completamente conectadas para crear los vectores de consulta, clave y valor.\n",
        ">* Las consultas y claves se someten a una multiplicación matricial de producto escalar para producir una matriz de puntuación, la cual determina cuanto énfasis se debe poner una palabra en otras palabras. Cuanto mayor sea la puntuación, mayor será el enfoque. Así es cómo se asignan las consultas a las claves.\n",
        ">* Estas puntuaciones se reducen dividiendo por la raíz cuadrada de la dimensión de la consulta y la clave para conseguir gradientes más estables.\n",
        ">* Se toma a continuación el **softmax** de la puntuación escalada para obtener las ponderaciones de atención (valores entre 0 y 1). Así el modelo tiene más confianza sobre qué palabras atender.\n",
        ">* Con esta matriz, multiplicamos la salida de softmax con el vector de valor para obtener el vector de salida. Las puntuaciones softmax  más altas mantednrán el valor de las palabras que el modelo aprende como más importante. Luego introduce la salida de esto en una capa lineal para procesar.\n",
        ">* Como queremos que sea un cálculo de atención multidireccional, debemos dividir la consulta, clave y valor en N vectores antes de aplicar la autoatención. Cada proceso de autoatención se llama cabeza y produce un vector de salida el cual se concatenará formando un único vector antes de pasar por la capa lineal final. En teoría, cada cabeza aprendería algo diferente, lo que le da al modelo codificador más poder de representación.\n",
        "* Conexión residual. El vector de salida de atención de múltiples encabezados se agrega a la incrustación de entrada posicional original. Esto es lo que se conoce como **conexión residual**.\n",
        ">Ayudan a la red a entrenar pues permiten que los gradientes fluyan directamente a través de las redes\n",
        "* Red de alimentación directa puntual. La salida de la conexión residual pasa por una normalización de capa y se proyecta a través de una red de retroalimentación puntual (se usa para proyectar las salidas de atención, dándole una representación más rica) para su posterior procesamiento.\n",
        ">* La salida se agrega nuevamente a la entrada de la red de alimentación directa puntual y se normaliza aún más.\n",
        ">* Las normalizaciones de capas sirven para estabilizar la red, lo que resulta en una reducción sustancial de tiempo de entrenamiento.\n",
        "* Todo este proceso tiene busca codificar la entrada en una representación continua con información de atención. Esto ayudará al decodificador a centrarse en las palabras apropiadas en la entrada. Se puede apilar el codificador N veces para codificar aún más la información, y así cada capa puede aprender diferentes representaciones de atención, lo que aumenta el poder predictivo del **transformer**."
      ],
      "metadata": {
        "id": "UtKEPjXgcDG6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. **Capa decodificadora**<br>\n",
        "Su objetivo es generar secuencias de texto. Es autorregresivo pues comienza con un token de inicio y toma como entradas una lista de salidas anteriores, así como las salidas del codificador que contienen la información de la atención de la entrada. Dejará de decodificar cuando genera un token como salida.<br>\n",
        "Su estructura es similar a la del codificador, pues tiene dos capas de atención multidireccional, una capa de avance puntual y conexiones residuales, y normalización de capas después de cada subcapa. Se comportan de manera similar a las del codificador, pero cada capa de atención tiene un trabajo diferente. Veamos las distintas capas del decodificador.\n",
        "* Incrustaciones de entrada y codificación posicional. Comienza prácticamente igual que el codificador. La entrada pasa por una capa de incrustación y una capa de codificación posicional para obtener incrustaciones posicionales. Estas se introducen en la primera capa de atención multidireccional que calula las puntuaciones de atención para la entrada del decodificador.\n",
        "* Atención multidireccional. En este caso, debemos distinguir entre la primera y la segundam atención multidireccional ya que su funcionamiento si varía respecto al visto anteriormente con los codificadores.\n",
        ">* Primera capa. Debido a la naturaleza autorregresiva del decodificador y que genera la secuancia palabra por palabra, necesitamos que no condicione a tokens futuros. Las palabras sólo pueden tener acceso a sí mismas y a palabras anteriores. Para evitar calcular puntuaciones de atención para palabras futuras usamos un método llamado **enmascaramiento**. Veamos cómo funciona.\n",
        ">>* Máscara de anticipación. Es una matriz del mismo tamaño que las puntuaciones de atención llena de valores nulos e infinitos negativos.\n",
        ">>* Se agrega antes de calcular el softmax y después de escalar las puntuaciones.\n",
        ">>* Al agregar la máscara a las puntuaciones, se obtiene una matriz de puntuaciones con el triángulo superior derecho lleno de infinitos negativos.\n",
        ">>* Así se consigue que al hacer el softmax, los infinitos se pongan a cero, por lo que el modelo no se va a centrar nunca en los tokens futuros.\n",
        ">>* Este enmascaramiento es la única diferencia en cómo se calculan las puntuaciones de atención en la primera capa.\n",
        ">>* Esta capa todavía tiene múltiples cabezales a los que se aplica la máscara, antes de concatenarse y pasar a través de una capa lineal para su posterior procesamiento. Su salida es un vector de salida enmascarado con información sobre cómo debe atender el modelo en la entrada del decodificador.\n",
        ">*Segunda capa. Para esta capa, las salidas del codificador son las consultas y las claves, y las salidas de la primera capa son los valores. Esto hace que la entrada del codificador y del decodificador coincidan, lo que permite al decodificador decidir en qué entrada del codificador centrarse.\n",
        ">>* Su salida pasa a través de una capa de avance puntual para su posterior procesamiento.\n",
        "* Clasificador lineal. Por ella pasa la salida de la capa final de avance puntual sobre la que actúa como clasificador.\n",
        "* SoftMax. La salida del clasificador se introduce en esta capa, que producirá puntuaciones de probabilidad (en el intervalo [0, 1]). Tomamos el índice de la puntuación de probabilidad más alta, y eso es igual a nuestra palabra predicha.\n",
        "* El decodificador toma la salida, la agrega a la lista de entradas del decodificador y continúa decodificando nuevamente hasta que se predice un token. Para nuestro caso, la predicción de mayor probabilidad es la clase final que se asigna al token final.\n",
        "* Como vimos anteriormente, también se pueden apilar N capas para el decodificador. Al apilarlas, el modelo puede aprender a extraer y centrarse en diferentes combinaciones de atención de sus cabezas de atención, lo que puede aumentar su poder predictivo."
      ],
      "metadata": {
        "id": "05XbzlOUcGDz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hemos visto que el modelo **transformer** original funciona como un modelo **Seq2Seq**. Sin embargo, depende el problema al que nos enfrentemos, se pueden utilizar tanto para la autorregresión como para el autoencoding. Veamos el funcionamiento de los **transfomers** en cada uno de los casos.\n",
        "* **Transformers AR**<br>\n",
        "Estos están fuertemente inspirados en el segmento decodificador del **Transformer** original. Son muy buenos cuando el objetivo es modelar el lenguaje (Natural Language Generation)\n",
        ">* Se incrusta la entrada. Esta incrustación es una matriz (matriz de icnrustación de posición), por lo que la entrada real es un vector con múltiples tokens (se puede usar una y otra vez).\n",
        ">* N (en este caso N = 12) segmentos de decodificador con segmentos de atención de múltiples cabezales enmascarados, segmentos de avance y segmentos de normalización de capas interpretan los valores de entrada.\n",
        ">* Mediante el preentrenamiento, el modelo aprende a modelar el lenguaje (NLG). Se puede ajustar posteriormente para las tareas detección de similitudes o respuestas de opción múltiple.\n",
        ">* El ejemplo más claro son la clase de **Transformers** llamada **GPT**.\n",
        "><p align=\"center\">\n",
        " <img width=\"500\" height=\"500\" src=\"https://github.com/christianversloot/machine-learning-articles/raw/main/images/Diagram-37.png\">\n",
        "</p>\n",
        "* **Transformers AE**<br>\n",
        "Hemos visto como los **modelos AR** son muy buenos cuando queremos modelar el lenguaje. Sin embargo, existen tareas que no se benefician ni de los **modelos AR** ni de los modelos **Seq2Seq**. Hablamos de actividades de comprensión del lenguaje natural (NLU).\n",
        ">* Los modelos **Seq2Seq** son necesarios para comprender el lenguaje pero utilizan esta comprensón para realizar una tarea diferente (generalmente la traducción).\n",
        ">* Los **modelos AR** no necesariamente requieren comprender el lenguaje si la generación se puede realizar con éxito (NLG).<br>\n",
        ">*Debido a estas limitaciones, los **modelos AE** pueden ayudar a la hora de desempeñar estas tareas. Anteriormente vimos como funcionan los **modelos AE**, tenemos entonces que  los **Transformers AE** corrompen las entradas y tienen como objetivo predecir las entradas originales para así aprender una codificación que puede usarse para tareas posteriores.\n",
        ">* Un ejemplo son los modelos tipo **BERT**, sobre los cuales hablaremos en detalle más adelante."
      ],
      "metadata": {
        "id": "C2HQeX3o1cty"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Teniendo en cuenta todo lo que hemos visto en esta sección sobre los **Transformers**, es la tarea que resuelve y el tipo de entrenamiento que se lleva a cabo lo que hace que pertenezca a uno u otro tipo, y no su arquitectura, pues el segmento decodificador del **Transformer** original se usa tradicionalmente para tareas AR pero se puede usar también para AE, y lo mismo ocurra con el segmento codificador.\n",
        "* Si se quiere que el modelo transformer sin alterar la semántica, hablamos de un modelo **Seq2Seq**.\n",
        "* Si la idea es que el modelo aprenda una representación codificada de las antradas corrompiéndlas y generando las variante originales, hablamos de un **modelo AE**.\n",
        "* Si buscamos utilizar todas las predicciones anteriores para generar la siguiente, estamos hablando de un **modelo AR**."
      ],
      "metadata": {
        "id": "cegKK2k64NLD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **MÉTODOS PARA *WORD EMBEDDING***"
      ],
      "metadata": {
        "id": "N5BqrWrU1i93"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Una vez que hemos visto las principales diferencias, veamos los distintos métodos que existen para cada uno de ellos. Comencemos con métodos usados para *word embedding*.<br>\n",
        "1. **Word2Vec (Word To Vectors)**: se basa en la estimación eficiente de palabras en un espacio vectorial y representaciones distribuidas de palabras y frases y su composicionalidad (significado de una expresión se forma a partir de las significados de sus partes y cómo están unidas). <br>Se centra en la idea de que una palabra puede representarse mediante un conjunto de palabras que aparecen cerca. Así, se suelen usar dos vectores para representar las características de una palabra (palabra central *V* y palabra de contexto *U*). Encontramos dos modelos distintos de redes neuronales en los que se basa este método.\n",
        ">*  **Skip-gram**: predice la palabra de contexto dada la palabra central. Busca maximizar la probabilidad de la palabra de contexto dada la palabra central. La idea es que dada la palabra central la probabilidad de que aparezca la palabra de contexto es la posibilidad de que ambas aparezcan juntas sobre todas las posibilidades entre todo el vocabulario\n",
        ">* **CBOW (Continuous Bag of Words)**: predice la palabra central dada una bolsa de palabras de contexto.\n",
        ">* Trabajamos con el descenso del gradiente etstocástico, para minimizar la pérdida y con el muestreo negativo, para evitar tener que recorrer el vocabulario todo el rato.\n",
        ">* **Skip-gram** tiene un rendimiento ligeramente mejor que **CBOW** y se utiliza con más frecuencia en **NLP**.\n",
        "2. **GloVe (Global Vectors for word representation)**: combina las ventajas de la factorización matricial global y la ventana de contexto local. La primera técnica utiliza las estadísticas del corpus (matriz de coocurrencia palabra-palabra) pero funciona mal en tareas de analogía de palabras. Mientras que los métodos de ventana de contexto local (**Skip-gram** o **CBOW**) lo hacen al revés. <br> Trata de extraer el significado de las palabras a través de las proporciones de probabilidades de coocurrencia entre palabras. Es conocido por capturar relaciones semánticas entre palabras."
      ],
      "metadata": {
        "id": "dehp22qUho0l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **MÉTODOS PARA *TEXTUAL EMBEDDING***"
      ],
      "metadata": {
        "id": "brKZgySf1ulT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Por otro lado, vamos a ver ahora algunos de los métodos usados para *textual embedding*. Tanto **ELMo** como **GPT-2** son modelos auto-regresivos.<br>\n",
        "1. **ELMo (Embeddings from Language Models)**: hablamos en este caso de una red neuronal profunda basada en LSTM (Long Short-Term Memory), que es bidireccional y genera representaciones contextuales de las palabras. Veamos sus principales características.\n",
        ">* **Entrada de palabras**: asigna un vector básico a cada palabra. Puede ser inicializado aleatoriamente o usando representaciones pre-entrenadas.\n",
        ">* **Capas LSTM bidireccionales**: cada palabra se procesa considerando tanto el contexto previo como el contexto posterior en la oración. Capturan las dependencias de largo alcance entre las palabras.\n",
        ">* **Combinación de representaciones**: se combinan las salidas de las capas en una representación final (combinación ponderada de las capas). Así, captura la información contextual en distintos niveles de abstracción.\n",
        ">* **Embeddings contextuales**: son las representaciones resultantes. Capturan la información semántica y contextual de cada palabra en función del contexto en el que aparece.<br>\n",
        ">* No es capaz de considerar información contextual de izquierda a derecha y de derecha a izquierda al mismo tiempo, ya que su mecanismo es predecir la siguiente palabra dadas las palabras anteriores.\n",
        "<br>\n",
        "\n",
        "2. **GPT-2 (Generative Pre-Trained Transformer 2)**: es conocido por su capacidad para generar texto coherente y cohesivo en lenguaje natural. Se basa en la arquitectura de **transformers** (hablaremos sobre ellos más adelante) y se pre-entrena en una gran cantidad de texto en varios idiomas antes de ser ajustado para llevar a cabo tareas específicas de NLP.\n",
        ">* Este modelo en su entrenamiento aprende a predecir la siguiente palabra en una oración dada una serie de palabras anteriores. Esto le permite comprender la estructura gramatical, la coherencia y el contexto en una amplia variedad de temas y estilos de estructura.\n",
        ">* Un componente especial del codificador de **Transformer** que usa es su atención enmascarada. Esto significa que la posición *i* sólo tiene en cuenta posiciones más pequeñas que *i*.\n",
        ">* **GPT-2** tiene un mejor rendimiento que **ELMo** debido a su arquitectura **Transformer** y su tamaño mucho mayor.\n",
        "<br>\n",
        "\n",
        "3. **BERT (Bidirectional Encoder Representations from Transformers)**: se basa en una red neuronal profunda altamente paralela, que se conoce como **transformers**. Su principal contribución es entrenar la incrustación de palabras basándose en codificadores automáticos de eliminación de ruido. Veamos sus principales características.\n",
        ">* **Entrada de texto**: divide el texto en tokens. Añade un token especial al principio y al final si hay varias oraciones.\n",
        ">* **Codificadores transformers**: cada codificador realiza múltiples capas de atención y feed forward para capturar relaciones y generar representaciones contextuales.\n",
        ">* **Embedding especial**: el token del principio (conocido como **CLS**) genera una representación especial que captura información sobre todo el texto de entrada.\n",
        ">* **Máscara de atención bidireccional**: sirve para predecir palabras ocultas en el texto. Permite comprender el contexto de la izquierda y la derecha de la palabra.\n",
        ">* **Embeddings contextuales**: capturan el significado contextual de los tokens en el texto. Es decir, permite incorporar información contextual de ambas direcciones al mismo tiempo.\n",
        ">* El entrenamiento del modelo tiene dos tareas, modelo de lenguaje enmascarado (MLM) y predicción de la siguiente oración (NSP).\n",
        "<br>\n",
        "4. **ALBERT (A Lite BERT)**: este modelo se basa en la arquitectura de **BERT** y está diseñado para ser más eficiente y escalable. Busca reducir la cantidad de parámetros necesarios para lograr un rendimiento parecido o superior al de **BERT**. Veamos a continuación algunas de sus características clave.\n",
        ">* **Factorización de parámetros**: en lugar de tener una gran cantidad de parámetros independientes para cada capa del modelo, comparte parámetros en diferentes capas. Esto reduce significativamente el número total de parámetros en el modelo, lo que lo hace más eficiente y permite entrenar modelos más grandes con menos recursos.\n",
        ">* **Capas de atención compartidas**: utiliza una estrategia de compartición de capas de atención entre las capas del modelo, lo que reduce la cantidad de parámetros y el costo computacional de la atención multi-cabeza en comparación con **BERT**.\n",
        ">* **Escalabilidad**: ha sido diseñado para ser escalable en términos de capacidad de entrenamiento y capacidad de representación. Permite agregar hasta 10 veces más datos de entrenamiento, lo que lo hace adecuado para aplicaciones a gran escala.\n",
        ">* **Eficiencia de recursos**: a pesar de la reducción de parámetros, ha demostrado tener un rendimiento competitivo en tarea de NLP. Por tanto, puede ser usado en entornos con recursos limitados.\n",
        ">* Una de las principales diferencias con **BERT**, es que en vez de usar **NSP** se centra en predecir el orden de las oracions (**SOP**), y reduce también el abandono en la tarea **MLM**.\n",
        "<br>\n",
        "5. **XLNET (eXtreme Learning NETwork)**: se trata también de una optimización de **BERT**. Vamos a ver las características que lo definen.\n",
        ">* **Modelo autoregresivo y de lenguaje**: se entrena para predecir la siguiente palabra en una secuencia dada una serie de palabras anteriores.\n",
        ">* **Permutación de palabras**: en el pre-entrenamiento, en lugar de predecir palabras en orden secuencial, el modelo es pre-entrenado para predecir palabras en orden aleatorio. Esto le permite capturar relaciones de largo alcance en el texto, pues debe considerar todas las palabras en una oración, en lugar de depender sólo de las palabras anteriores, y así mitigar el inconveniente de los modelos autorregresivos.\n",
        ">* **Atención bidireccional**: funciona al igual que vimos en **BERT**, lo que le permite comprender y modelar las relaciones entre las palabras de manera más efectiva.\n",
        ">* **Rendimiento en tareas NLP**: ha demostrado un rendimiento excepcional en una variedad de tareas de NLP, superando a modelos previos en muchas de estas tareas debido a su capacidad para modelar relaciones de largo alcance y contextos complejos.\n",
        ">* **Arquitectura de transformer**: esta arquitectura utiliza mecanismos de atención para capturar relaciones de largo alcance en el texto. En lugar del **transformer** estándar, se utiliza también el siguiente para mejorar el rendimiento.\n",
        ">> **Transformer XL**: la idea principal es dividir una secuencia larga de entrada en un conjunto de secuencias más pequeñas para reducir el uso de memoria y mejorar la velocidad de paso de atención. Cada bloque transformador funciona en una secuencia pequeña. Una modificación clave es eliminar la codificación posicional estándar e incluir la codificación de posición relativa en la atención de múltiples cabezales."
      ],
      "metadata": {
        "id": "1xMZa6BieGoN"
      }
    }
  ]
}